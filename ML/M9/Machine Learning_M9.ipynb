{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPF47lcy424YBaw2pM39sJH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#MODULE_9_ML: ASSIGNMENT."],"metadata":{"id":"xuhcG3OGe19D"}},{"cell_type":"markdown","source":["#Ensemble Learning Assignment Questions"],"metadata":{"id":"uo9f8-1aestU"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"IVGerAHxe6BS"}},{"cell_type":"markdown","source":["\n","# Machine Learning Questions and Answers\n","\n","## 1. Can we use Bagging for regression problems?\n","- Yes, Bagging can be used for regression problems.\n","- In regression, the model's output is determined by averaging the predictions of all models.\n","- The method reduces variance by combining multiple predictions.\n","- Bagging helps in making the model less sensitive to noise in the data.\n","- It can be particularly useful for high-variance regression models, such as Decision Trees.\n","- For regression, the algorithm computes the average of predictions made by individual models.\n","\n","## 2. What is the difference between multiple model training and single model training?\n","- Multiple model training involves training several models independently and combining their results.\n","- Each model can be of the same type (e.g., multiple Decision Trees) or different types (e.g., decision trees, neural networks, etc.).\n","- Single model training involves training only one model to make predictions.\n","- Multiple model training improves performance by averaging out biases and reducing variance.\n","- Single model training is simpler but may suffer from overfitting or underfitting.\n","- Multiple model training is common in ensemble methods like Bagging and Boosting.\n","\n","## 3. Explain the concept of feature randomness in Random Forest.\n","- Feature randomness in Random Forest refers to selecting a random subset of features at each split of a decision tree.\n","- This randomness helps in reducing correlation between trees, making the model more diverse.\n","- It increases the robustness of the model by reducing overfitting.\n","- Feature randomness ensures that each tree is less likely to overfit to specific features.\n","- The goal is to make trees that are diverse and less prone to errors due to certain features.\n","- It allows Random Forest to generalize better than a single decision tree.\n","\n","## 4. What is OOB (Out-of-Bag) Score?\n","- OOB score is an estimate of the performance of a Random Forest model without needing a separate validation set.\n","- It uses the data points that were not included in the bootstrap sample for a given tree to test that tree.\n","- OOB error is the average prediction error on the out-of-bag samples.\n","- The OOB score provides a quick estimate of model performance during training.\n","- It can be used to tune hyperparameters without cross-validation.\n","- OOB can be particularly useful when data is limited.\n","\n","## 5. How can you measure the importance of features in a Random Forest model?\n","- Feature importance in Random Forest can be measured using the Gini index or entropy.\n","- The Gini index is calculated at each split, and the importance is based on how much each feature reduces impurity.\n","- Another method is using mean decrease accuracy, which measures how much the model’s accuracy decreases when a feature is removed.\n","- Feature importance can be visualized using bar plots to show the relative contribution of each feature.\n","- It helps in identifying the most influential features for prediction.\n","- Feature importance can guide feature selection, leading to better model performance.\n","\n","## 6. Explain the working principle of a Bagging Classifier.\n","- Bagging stands for Bootstrap Aggregating, where multiple models are trained on bootstrapped subsets of the training data.\n","- Each model is trained independently on a random subset of data.\n","- For classification, the final prediction is determined by majority voting among the models.\n","- Bagging reduces the variance of high-variance models like Decision Trees.\n","- It works well for models that are prone to overfitting.\n","- Bagging allows parallel training since models do not depend on each other.\n","\n","## 7. How do you evaluate a Bagging Classifier’s performance?\n","- Performance can be evaluated using accuracy, precision, recall, F1-score, or area under the ROC curve for classification tasks.\n","- Cross-validation or OOB score can provide an estimate of generalization error.\n","- A confusion matrix can be used to visualize the performance in terms of true positives, false positives, etc.\n","- For regression tasks, performance can be measured using mean squared error (MSE) or R-squared.\n","- It's essential to evaluate the classifier on a test set that was not used for training.\n","- Metrics should align with the problem's specific goals (e.g., minimizing false positives in fraud detection).\n","\n","## 8. How does a Bagging Regressor work?\n","- A Bagging Regressor is similar to a Bagging Classifier, but it handles regression tasks.\n","- Multiple models are trained on bootstrapped subsets of the data.\n","- Instead of majority voting, the final output is the average of all model predictions.\n","- Bagging helps reduce variance, especially for high-variance models like Decision Trees.\n","- It works well for regression models that tend to overfit to the training data.\n","- The aggregation of predictions from multiple models improves generalization.\n","\n","## 9. What is the main advantage of ensemble techniques?\n","- Ensemble techniques combine the predictions of multiple models to improve overall accuracy.\n","- They help in reducing overfitting by averaging out errors from individual models.\n","- Ensembles can reduce both variance (increasing robustness) and bias (improving accuracy).\n","- They are particularly useful when individual models have high variance or bias.\n","- The strength of ensemble methods lies in combining the strengths of diverse models.\n","- They can improve predictions without needing a drastic change in individual model architecture.\n","\n","## 10. What is the main challenge of ensemble methods?\n","- One main challenge is the increased computational cost due to training multiple models.\n","- Ensembles may be harder to interpret compared to a single model, leading to a lack of transparency.\n","- Combining multiple models can increase the complexity of the overall system.\n","- Overfitting can still occur if the individual models in the ensemble are not diverse enough.\n","- Tuning ensemble models requires careful selection of base models and their parameters.\n","- Managing memory and processing time for large ensembles can be challenging.\n","\n","## 11. Explain the key idea behind ensemble techniques.\n","- Ensemble techniques combine multiple base models to create a stronger overall model.\n","- The idea is that combining several models can improve accuracy and robustness.\n","- By averaging predictions or using majority voting, ensemble methods reduce overfitting.\n","- Different models may have different biases, and averaging them can help cancel out errors.\n","- The diversity of models in the ensemble is key to its effectiveness.\n","- Ensemble methods are widely used to solve complex problems in machine learning.\n","\n","## 12. What is a Random Forest Classifier?\n","- A Random Forest Classifier is an ensemble learning method based on Bagging.\n","- It uses multiple decision trees to classify data, each trained on a random subset of data.\n","- The final prediction is determined by the majority vote of the trees.\n","- Random Forest improves accuracy by reducing variance compared to a single decision tree.\n","- It is highly robust and handles missing values and overfitting well.\n","- Random Forest can also provide feature importance, helping to identify key features for classification.\n","\n","## 13. What are the main types of ensemble techniques?\n","- The main types of ensemble techniques are:\n","  - **Bagging**: Trains models in parallel and combines their results by averaging (regression) or voting (classification).\n","  - **Boosting**: Trains models sequentially, where each model corrects the errors of the previous one.\n","  - **Stacking**: Combines multiple models by training a new model to predict the final output based on the predictions of base models.\n","- Each technique has different approaches to combining models and improving accuracy.\n","\n","## 14. What is ensemble learning in machine learning?\n","- Ensemble learning is the process of combining multiple models to create a stronger, more accurate model.\n","- The idea is to leverage the strengths of different models to improve predictions.\n","- The ensemble model usually outperforms individual models because it reduces errors from individual predictions.\n","- It can be used to solve a variety of tasks, including classification, regression, and ranking problems.\n","- The models can be of the same type (e.g., multiple decision trees) or different types.\n","- Ensemble learning is particularly useful for high-variance or high-bias models.\n","\n","## 15. When should we avoid using ensemble methods?\n","- Ensemble methods should be avoided when computational resources are limited due to the overhead of training multiple models.\n","- They should also be avoided in cases where model interpretability is critical, as ensembles can be harder to understand.\n","- If the problem does not require high performance or improvement over single models, ensemble methods may not be necessary.\n","- Ensemble methods may also be unnecessary when individual models already provide satisfactory performance.\n","- In real-time systems, the computational cost of ensemble methods may introduce unacceptable delays.\n","- When data is small and overfitting is not a concern, simpler models may suffice.\n","\n","## 16. How does Bagging help in reducing overfitting?\n","- Bagging reduces overfitting by averaging multiple models trained on different subsets of the data.\n","- It reduces the impact of noise and outliers by combining predictions from several models.\n","- Since each model is trained on a different subset, it is less likely to overfit to the specific quirks of the data.\n","- Bagging helps in creating a more generalizable model by decreasing variance.\n","- The models in Bagging are less prone to fitting random fluctuations in the training data.\n","- Bagging improves robustness by increasing the diversity of models in the ensemble.\n","\n","## 17. Why is Random Forest better than a single Decision Tree?\n","- Random Forest improves upon a single Decision Tree by averaging the predictions from multiple trees, reducing variance.\n","- It reduces the risk of overfitting, which is a common issue with individual decision trees.\n","- Random Forest can handle a larger variety of data and is more robust to outliers.\n","- The ensemble nature of Random Forest makes it more accurate and stable.\n","- It uses feature randomness to reduce correlation between trees, increasing model diversity.\n","- Unlike a single Decision Tree, Random Forest provides feature importance, aiding in feature selection.\n","\n","## 18. What is the role of bootstrap sampling in Bagging?\n","- Bootstrap sampling involves randomly selecting subsets of the data with replacement to train multiple models.\n","- It allows for training models on slightly different data, increasing model diversity.\n","- The process helps in reducing variance by averaging the results of multiple models.\n","- Bootstrap sampling ensures that each model is trained on slightly different subsets of data, making each model independent.\n","- It helps in handling missing data, as some data points may be used multiple times in the bootstrapped samples.\n","- This technique is the foundation of Bagging algorithms like Random Forest.\n","\n","## 19. What are some real-world applications of ensemble techniques?\n","- Ensemble techniques are used in fraud detection to combine multiple models' predictions for better accuracy.\n","- They are widely used in recommendation systems to improve the relevance of suggestions.\n","- In medical diagnosis, ensembles can combine predictions from various models to improve decision-making.\n","- Image classification tasks benefit from ensemble methods to improve the robustness of the classifier.\n","- Ensemble methods are also used in financial modeling, particularly in stock market predictions.\n","- They are commonly used in text classification tasks, such as spam filtering and sentiment analysis.\n","\n","## 20. What is the difference between Bagging and Boosting?\n","- Bagging trains multiple models in parallel using bootstrapped subsets of the data and combines their results through averaging or voting.\n","- Boosting trains models sequentially, with each model focusing on correcting the errors of the previous one.\n","- Bagging is typically used for reducing variance, while Boosting aims to reduce bias.\n","- Bagging involves models that are trained independently, whereas Boosting models are dependent on each other.\n","- Boosting can lead to better performance but may be more prone to overfitting compared to Bagging.\n","- Bagging can be used with high-variance models, while Boosting is often used to improve weak learners.\n"],"metadata":{"id":"PuuB_x6NicDQ"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"uYpJ4wr0kwRw"}},{"cell_type":"markdown","source":["###Practical Question"],"metadata":{"id":"shNuhbFJkqPb"}},{"cell_type":"markdown","source":["---"],"metadata":{"id":"yIE8rmtrkuRz"}},{"cell_type":"code","source":["# 21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n","from sklearn.datasets import load_iris\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Bagging Classifier using Decision Trees\n","# Updated 'base_estimator' to 'estimator' for compatibility with newer sklearn versions\n","bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n","\n","# Train the model\n","bagging_model.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = bagging_model.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Bagging Classifier Accuracy: {accuracy:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t_oxFW09ez8k","executionInfo":{"status":"ok","timestamp":1747256137716,"user_tz":-330,"elapsed":1692,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"43a68509-1908-4de5-847f-a9dd0d6985da"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Classifier Accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["# 22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n","from sklearn.metrics import mean_squared_error\n","from sklearn.ensemble import BaggingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","\n","# Load a sample regression dataset\n","X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Bagging Regressor using Decision Trees\n","bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, random_state=42)\n","\n","# Train the model\n","bagging_regressor.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = bagging_regressor.predict(X_test)\n","\n","# Calculate MSE\n","mse = mean_squared_error(y_test, y_pred)\n","print(f\"Bagging Regressor MSE: {mse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q41i5oUFkx06","executionInfo":{"status":"ok","timestamp":1747256184853,"user_tz":-330,"elapsed":5,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"0b4ce6c2-9c6e-4a39-cfdf-d4b1c3e24297"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Regressor MSE: 438.8215\n"]}]},{"cell_type":"code","source":["# 23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","\n","# Load Breast Cancer dataset\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Random Forest Classifier\n","rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n","\n","# Train the model\n","rf_classifier.fit(X_train, y_train)\n","\n","# Print feature importance scores\n","importances = rf_classifier.feature_importances_\n","print(f\"Feature Importances: {importances}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pdq8OSzHlRfh","executionInfo":{"status":"ok","timestamp":1747256220271,"user_tz":-330,"elapsed":633,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"f537bc2e-f76e-4271-c29d-b3268d39f80a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Importances: [0.03231189 0.0110639  0.06009233 0.05381045 0.00622336 0.00921566\n"," 0.08055702 0.14193444 0.00327807 0.00314028 0.01643496 0.00317191\n"," 0.01176976 0.02953842 0.00588079 0.00459638 0.0058159  0.00338232\n"," 0.00400077 0.00713457 0.07797475 0.01878567 0.07429212 0.11821686\n"," 0.01176917 0.01753909 0.04107958 0.12713638 0.01292945 0.00692376]\n"]}]},{"cell_type":"code","source":["# 24. Train a Random Forest Regressor and compare its performance with a single Decision Tree\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","from sklearn.ensemble import RandomForestRegressor\n","\n","# Load regression dataset\n","X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Random Forest Regressor\n","rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n","\n","# Create a Decision Tree Regressor\n","dt_regressor = DecisionTreeRegressor(random_state=42)\n","\n","# Train both models\n","rf_regressor.fit(X_train, y_train)\n","dt_regressor.fit(X_train, y_train)\n","\n","# Predict on test set\n","rf_pred = rf_regressor.predict(X_test)\n","dt_pred = dt_regressor.predict(X_test)\n","\n","# Calculate MSE for both models\n","rf_mse = mean_squared_error(y_test, rf_pred)\n","dt_mse = mean_squared_error(y_test, dt_pred)\n","\n","print(f\"Random Forest Regressor MSE: {rf_mse:.4f}\")\n","print(f\"Decision Tree Regressor MSE: {dt_mse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e2ogt9TplhLD","executionInfo":{"status":"ok","timestamp":1747256226224,"user_tz":-330,"elapsed":441,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"b35b074c-5941-45c8-f5cf-92a8b54bd529"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest Regressor MSE: 377.2493\n","Decision Tree Regressor MSE: 945.0730\n"]}]},{"cell_type":"code","source":["# 25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_breast_cancer\n","from sklearn.model_selection import train_test_split\n","\n","# Load Breast Cancer dataset\n","data = load_breast_cancer()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Random Forest Classifier with OOB score enabled\n","rf_classifier_oob = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n","\n","# Train the model\n","rf_classifier_oob.fit(X_train, y_train)\n","\n","# Print OOB score\n","print(f\"OOB Score: {rf_classifier_oob.oob_score_:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a1w8pw0nliut","executionInfo":{"status":"ok","timestamp":1747256234212,"user_tz":-330,"elapsed":524,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"2f1fbda5-76ba-4354-cd83-5484b58fa539"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["OOB Score: 0.9548\n"]}]},{"cell_type":"code","source":["# 26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n","from sklearn.svm import SVC\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Bagging Classifier using SVM\n","bagging_svm = BaggingClassifier(estimator=SVC(), n_estimators=50, random_state=42)\n","\n","# Train the model\n","bagging_svm.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_svm = bagging_svm.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy_svm = accuracy_score(y_test, y_pred_svm)\n","print(f\"Bagging Classifier with SVM Accuracy: {accuracy_svm:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"as3K1Jvslkq2","executionInfo":{"status":"ok","timestamp":1747256252924,"user_tz":-330,"elapsed":610,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"13a3f28a-ed00-4d82-a663-6a889438ea50"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Classifier with SVM Accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["# 27. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train Random Forest Classifier with different numbers of trees and compare accuracies\n","accuracies = []\n","tree_counts = [10, 50, 100, 200]\n","\n","for n_trees in tree_counts:\n","    rf_classifier = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n","    rf_classifier.fit(X_train, y_train)\n","    y_pred_rf = rf_classifier.predict(X_test)\n","    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n","    accuracies.append(accuracy_rf)\n","    print(f\"Random Forest with {n_trees} Trees Accuracy: {accuracy_rf:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o8IL-iMjlmAm","executionInfo":{"status":"ok","timestamp":1747256248266,"user_tz":-330,"elapsed":1987,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"990e81cb-3bad-45f8-b37e-7fea965428bf"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest with 10 Trees Accuracy: 1.0000\n","Random Forest with 50 Trees Accuracy: 1.0000\n","Random Forest with 100 Trees Accuracy: 1.0000\n","Random Forest with 200 Trees Accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["# 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.metrics import roc_auc_score\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Bagging Classifier using Logistic Regression\n","bagging_logreg = BaggingClassifier(estimator=LogisticRegression(solver='liblinear'), n_estimators=50, random_state=42)\n","\n","# Train the model\n","bagging_logreg.fit(X_train, y_train)\n","\n","# Predict on test set\n","# Get probabilities for all classes\n","y_pred_logreg_proba = bagging_logreg.predict_proba(X_test)\n","\n","# Calculate AUC score using the 'ovr' multi-class strategy\n","auc_score = roc_auc_score(y_test, y_pred_logreg_proba, multi_class='ovr')\n","print(f\"Bagging Classifier with Logistic Regression AUC (OvR): {auc_score:.4f}\")\n","\n","auc_score_ovo = roc_auc_score(y_test, y_pred_logreg_proba, multi_class='ovo')\n","print(f\"Bagging Classifier with Logistic Regression AUC (OvO): {auc_score_ovo:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NSF-PneHlnv0","executionInfo":{"status":"ok","timestamp":1747256708333,"user_tz":-330,"elapsed":378,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"7ec2fb83-06fe-45d4-870e-1eedadcd4c73"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Classifier with Logistic Regression AUC (OvR): 1.0000\n","Bagging Classifier with Logistic Regression AUC (OvO): 1.0000\n"]}]},{"cell_type":"code","source":["# 29. Train a Random Forest Regressor and analyze feature importance scores\n","from sklearn.ensemble import RandomForestRegressor\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","\n","# Load regression dataset\n","X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create and train Random Forest Regressor\n","rf_regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n","rf_regressor.fit(X_train, y_train)\n","\n","# Print feature importance scores\n","rf_importances = rf_regressor.feature_importances_\n","print(f\"Feature Importances for Random Forest Regressor: {rf_importances}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xO_-VxuAlr13","executionInfo":{"status":"ok","timestamp":1747256271452,"user_tz":-330,"elapsed":382,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"e9d226b9-6ceb-4aaf-940b-d23e1c3edd1f"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Feature Importances for Random Forest Regressor: [0.55082174 0.44917826]\n"]}]},{"cell_type":"code","source":["# 30. Train an ensemble model using both Bagging and Random Forest and compare accuracy.\n","from sklearn.metrics import accuracy_score\n","from sklearn.ensemble import BaggingClassifier, RandomForestClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Train Bagging Classifier with Decision Trees\n","bagging_model = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n","bagging_model.fit(X_train, y_train)\n","y_pred_bagging = bagging_model.predict(X_test)\n","bagging_accuracy = accuracy_score(y_test, y_pred_bagging)\n","\n","# Train Random Forest Classifier\n","rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_classifier.fit(X_train, y_train)\n","y_pred_rf = rf_classifier.predict(X_test)\n","rf_accuracy = accuracy_score(y_test, y_pred_rf)\n","\n","print(f\"Bagging Classifier Accuracy: {bagging_accuracy:.4f}\")\n","print(f\"Random Forest Classifier Accuracy: {rf_accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IJ7LGz00ltyg","executionInfo":{"status":"ok","timestamp":1747256294372,"user_tz":-330,"elapsed":634,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"f2b64312-b06f-49aa-aa9d-a33e86a0f92a"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Classifier Accuracy: 1.0000\n","Random Forest Classifier Accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["# 31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define parameter grid for Random Forest Classifier\n","param_grid = {\n","    'n_estimators': [50, 100, 200],\n","    'max_depth': [10, 20, None],\n","    'min_samples_split': [2, 5, 10]\n","}\n","\n","# Create a Random Forest Classifier\n","rf_classifier = RandomForestClassifier(random_state=42)\n","\n","# Perform GridSearchCV\n","grid_search = GridSearchCV(estimator=rf_classifier, param_grid=param_grid, cv=3, n_jobs=-1, verbose=2)\n","grid_search.fit(X_train, y_train)\n","\n","# Print best parameters and score\n","print(f\"Best parameters: {grid_search.best_params_}\")\n","print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zveKXatqlvJv","executionInfo":{"status":"ok","timestamp":1747256408710,"user_tz":-330,"elapsed":17657,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"131c8736-c6c8-419c-ee70-035ea05c36a0"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 3 folds for each of 27 candidates, totalling 81 fits\n","Best parameters: {'max_depth': 10, 'min_samples_split': 5, 'n_estimators': 200}\n","Best cross-validation score: 0.9524\n"]}]},{"cell_type":"code","source":["# 32. Train a Bagging Regressor with different numbers of base estimators and compare performance\n","from sklearn.ensemble import BaggingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","\n","# Load regression dataset\n","X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Evaluate performance for different number of base estimators in Bagging Regressor\n","mse_scores = []\n","estimators_range = [10, 50, 100]\n","\n","for n_estimators in estimators_range:\n","    # Create a Bagging Regressor\n","    bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=n_estimators, random_state=42)\n","    bagging_regressor.fit(X_train, y_train)\n","\n","    # Predict and calculate MSE\n","    y_pred = bagging_regressor.predict(X_test)\n","    mse = mean_squared_error(y_test, y_pred)\n","    mse_scores.append(mse)\n","    print(f\"Bagging Regressor with {n_estimators} Estimators - MSE: {mse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hXDfdjxqmLFu","executionInfo":{"status":"ok","timestamp":1747256664501,"user_tz":-330,"elapsed":369,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"aafbfe9a-c125-4b84-bb5d-d0a2b34f6e60"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Regressor with 10 Estimators - MSE: 716.5134\n","Bagging Regressor with 50 Estimators - MSE: 438.8215\n","Bagging Regressor with 100 Estimators - MSE: 367.8717\n"]}]},{"cell_type":"code","source":["# 33. Train a Random Forest Classifier and analyze misclassified samples\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Random Forest Classifier\n","rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n","\n","# Train the model\n","rf_classifier.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred = rf_classifier.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Random Forest Classifier Accuracy: {accuracy:.4f}\")\n","\n","# Identify misclassified samples\n","misclassified = X_test[y_pred != y_test]\n","print(f\"Misclassified Samples: {misclassified}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Tc9wh3hEmMtS","executionInfo":{"status":"ok","timestamp":1747256416786,"user_tz":-330,"elapsed":366,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"eec8511e-da7b-4a40-80f1-fce28d841499"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest Classifier Accuracy: 1.0000\n","Misclassified Samples: []\n"]}]},{"cell_type":"code","source":["# 34. Train a Bagging Classifier and compare its performance with a single Decision Tree Classifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create and train Bagging Classifier\n","bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n","bagging_classifier.fit(X_train, y_train)\n","\n","# Create and train a single Decision Tree Classifier\n","dt_classifier = DecisionTreeClassifier(random_state=42)\n","dt_classifier.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_bagging = bagging_classifier.predict(X_test)\n","y_pred_dt = dt_classifier.predict(X_test)\n","\n","# Calculate accuracy for both models\n","accuracy_bagging = accuracy_score(y_test, y_pred_bagging)\n","accuracy_dt = accuracy_score(y_test, y_pred_dt)\n","\n","print(f\"Bagging Classifier Accuracy: {accuracy_bagging:.4f}\")\n","print(f\"Decision Tree Classifier Accuracy: {accuracy_dt:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7XH8LMZAmRSW","executionInfo":{"status":"ok","timestamp":1747256647625,"user_tz":-330,"elapsed":10,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"88ddba4f-9b22-479c-fd4c-e01f01f5c80d"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Classifier Accuracy: 1.0000\n","Decision Tree Classifier Accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["# 35. Train a Random Forest Classifier and visualize the confusion matrix\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create and train Random Forest Classifier\n","rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_classifier.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_rf = rf_classifier.predict(X_test)\n","\n","# Generate confusion matrix\n","cm = confusion_matrix(y_test, y_pred_rf)\n","\n","# Visualize confusion matrix\n","disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n","disp.plot(cmap=plt.cm.Blues)\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":449},"id":"jSejXMTzmTYy","executionInfo":{"status":"ok","timestamp":1747256432220,"user_tz":-330,"elapsed":1096,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"ad36af44-70ff-4c35-bc31-44594a628fd3"},"execution_count":21,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 2 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# 36. Train a Stacking Classifier using Decision Trees, SVM, and Logistic Regression, and compare accuracy\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define base models for Stacking Classifier\n","base_learners = [\n","    ('decision_tree', DecisionTreeClassifier(random_state=42)),\n","    ('svm', SVC(probability=True, random_state=42)),\n","    ('logreg', LogisticRegression(random_state=42))\n","]\n","\n","# Create and train Stacking Classifier\n","stacking_classifier = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n","stacking_classifier.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_stack = stacking_classifier.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy_stack = accuracy_score(y_test, y_pred_stack)\n","print(f\"Stacking Classifier Accuracy: {accuracy_stack:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o4TTDcrTmU4d","executionInfo":{"status":"ok","timestamp":1747256442154,"user_tz":-330,"elapsed":988,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"a7bd5398-26bd-4f90-d549-2c3e82e79b89"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Stacking Classifier Accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["# 37. Train a Random Forest Classifier and print the top 5 most important features\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","feature_names = data.feature_names\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create and train Random Forest Classifier\n","rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_classifier.fit(X_train, y_train)\n","\n","# Get feature importances\n","importances = rf_classifier.feature_importances_\n","\n","# Get the indices of the top 5 most important features\n","top_5_indices = importances.argsort()[-5:][::-1]\n","top_5_features = [feature_names[i] for i in top_5_indices]\n","\n","print(f\"Top 5 Most Important Features: {top_5_features}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6302fn04mXT4","executionInfo":{"status":"ok","timestamp":1747256450888,"user_tz":-330,"elapsed":379,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"e84815ca-59a5-4adc-fb6c-38be061df696"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 5 Most Important Features: ['petal width (cm)', 'petal length (cm)', 'sepal length (cm)', 'sepal width (cm)']\n"]}]},{"cell_type":"code","source":["# 38. Train a Bagging Classifier and evaluate performance using Precision, Recall, and F1-score\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.metrics import precision_score, recall_score, f1_score\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Create a Bagging Classifier with Decision Tree as base estimator\n","bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n","bagging_classifier.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_bagging = bagging_classifier.predict(X_test)\n","\n","# Calculate Precision, Recall, and F1-score\n","precision = precision_score(y_test, y_pred_bagging, average='macro')\n","recall = recall_score(y_test, y_pred_bagging, average='macro')\n","f1 = f1_score(y_test, y_pred_bagging, average='macro')\n","\n","print(f\"Precision: {precision:.4f}\")\n","print(f\"Recall: {recall:.4f}\")\n","print(f\"F1-Score: {f1:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"59xXiE5JmZlx","executionInfo":{"status":"ok","timestamp":1747256631728,"user_tz":-330,"elapsed":10,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"970e76a7-b045-402a-b884-c4a27ea131e2"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["Precision: 1.0000\n","Recall: 1.0000\n","F1-Score: 1.0000\n"]}]},{"cell_type":"code","source":["# 39. Train a Random Forest Classifier and analyze the effect of max_depth on accuracy\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Evaluate performance for different max_depth values\n","max_depth_values = [3, 5, 10, None]\n","for max_depth in max_depth_values:\n","    # Create and train Random Forest Classifier with specified max_depth\n","    rf_classifier = RandomForestClassifier(n_estimators=100, max_depth=max_depth, random_state=42)\n","    rf_classifier.fit(X_train, y_train)\n","\n","    # Predict on test set\n","    y_pred_rf = rf_classifier.predict(X_test)\n","\n","    # Calculate accuracy\n","    accuracy = accuracy_score(y_test, y_pred_rf)\n","    print(f\"Random Forest with max_depth={max_depth} - Accuracy: {accuracy:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l5jcF_jEmfmB","executionInfo":{"status":"ok","timestamp":1747256482299,"user_tz":-330,"elapsed":1880,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"5d846686-9ead-4a38-e465-49fed66fdc0d"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest with max_depth=3 - Accuracy: 1.0000\n","Random Forest with max_depth=5 - Accuracy: 1.0000\n","Random Forest with max_depth=10 - Accuracy: 1.0000\n","Random Forest with max_depth=None - Accuracy: 1.0000\n"]}]},{"cell_type":"code","source":["# 40. Train a Bagging Regressor using different base estimators (DecisionTree and KNeighbors) and compare performance\n","from sklearn.ensemble import BaggingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.neighbors import KNeighborsRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","\n","# Load regression dataset\n","X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Evaluate performance for different base estimators in Bagging Regressor\n","base_estimators = [DecisionTreeRegressor(), KNeighborsRegressor()]\n","for base_estimator in base_estimators:\n","    # Create and train Bagging Regressor with specified base estimator\n","    bagging_regressor = BaggingRegressor(estimator=base_estimator, n_estimators=50, random_state=42)\n","    bagging_regressor.fit(X_train, y_train)\n","\n","    # Predict and calculate MSE\n","    y_pred = bagging_regressor.predict(X_test)\n","    mse = mean_squared_error(y_test, y_pred)\n","    print(f\"Bagging Regressor with {base_estimator.__class__.__name__} - MSE: {mse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GZxBthZXmg6H","executionInfo":{"status":"ok","timestamp":1747256613790,"user_tz":-330,"elapsed":651,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"cff7cc4e-e6e5-4e27-a630-f9ed35b35ebc"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Regressor with DecisionTreeRegressor - MSE: 438.8215\n","Bagging Regressor with KNeighborsRegressor - MSE: 428.4685\n"]}]},{"cell_type":"code","source":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NNJvaubPmiOv","executionInfo":{"status":"ok","timestamp":1747256493886,"user_tz":-330,"elapsed":379,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"bf8d9e7b-80bb-4044-b968-32ac7c0b2cc5"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Random Forest ROC-AUC Score: 1.0000\n"]}]},{"cell_type":"code","source":["# 42. Train a Bagging Classifier and evaluate its performance using cross-validation\n","from sklearn.ensemble import BaggingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import cross_val_score\n","from sklearn.datasets import load_iris\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Create a Bagging Classifier with Decision Tree as base estimator\n","bagging_classifier = BaggingClassifier(estimator=DecisionTreeClassifier(), n_estimators=50, random_state=42)\n","\n","# Evaluate the model using cross-validation\n","cv_scores = cross_val_score(bagging_classifier, X, y, cv=5)\n","\n","# Print cross-validation results\n","print(f\"Cross-validation scores: {cv_scores}\")\n","print(f\"Mean cross-validation score: {cv_scores.mean():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_CqDNe5WmkG1","executionInfo":{"status":"ok","timestamp":1747256586743,"user_tz":-330,"elapsed":892,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"ea2779b3-60bf-4ba8-df8a-828ba44b7ced"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["Cross-validation scores: [0.96666667 0.96666667 0.93333333 0.96666667 1.        ]\n","Mean cross-validation score: 0.9667\n"]}]},{"cell_type":"code","source":["# 43. Train a Random Forest Classifier and plot the Precision-Recall curve\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import precision_recall_curve\n","import matplotlib.pyplot as plt\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Convert the target to binary for Precision-Recall curve\n","y_binary = (y == 0).astype(int)  # Class 0 vs Rest\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.3, random_state=42)\n","\n","# Create and train Random Forest Classifier\n","rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_classifier.fit(X_train, y_train)\n","\n","# Predict probabilities on test set\n","y_pred_prob = rf_classifier.predict_proba(X_test)[:, 1]\n","\n","# Compute precision and recall\n","precision, recall, _ = precision_recall_curve(y_test, y_pred_prob)\n","\n","# Plot Precision-Recall curve\n","plt.plot(recall, precision, color='blue')\n","plt.xlabel('Recall')\n","plt.ylabel('Precision')\n","plt.title('Precision-Recall Curve')\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":472},"id":"pXs53HrxmlmC","executionInfo":{"status":"ok","timestamp":1747256506579,"user_tz":-330,"elapsed":511,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"ca1b4ca6-c1b7-4b6e-8211-bb8e283451bd"},"execution_count":29,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","source":["# 44. Train a Stacking Classifier with Random Forest and Logistic Regression and compare accuracy\n","from sklearn.ensemble import StackingClassifier\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score\n","from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","\n","# Load dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Define base models for Stacking Classifier\n","base_learners = [\n","    ('rf', RandomForestClassifier(n_estimators=50, random_state=42)),\n","    ('logreg', LogisticRegression(random_state=42))\n","]\n","\n","# Create and train Stacking Classifier\n","stacking_classifier = StackingClassifier(estimators=base_learners, final_estimator=LogisticRegression())\n","stacking_classifier.fit(X_train, y_train)\n","\n","# Predict on test set\n","y_pred_stack = stacking_classifier.predict(X_test)\n","\n","# Calculate accuracy\n","accuracy_stack = accuracy_score(y_test, y_pred_stack)\n","print(f\"Stacking Classifier Accuracy (RF + LogReg): {accuracy_stack:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tMOLWqkdmnLS","executionInfo":{"status":"ok","timestamp":1747256514002,"user_tz":-330,"elapsed":640,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"8e81e986-014e-49a8-f6eb-17293b795252"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Stacking Classifier Accuracy (RF + LogReg): 1.0000\n"]}]},{"cell_type":"code","source":["# 45. Train a Bagging Regressor with different levels of bootstrap samples and compare performance\n","from sklearn.ensemble import BaggingRegressor\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.metrics import mean_squared_error\n","from sklearn.datasets import make_regression\n","from sklearn.model_selection import train_test_split\n","\n","# Load regression dataset\n","X, y = make_regression(n_samples=100, n_features=2, noise=0.1, random_state=42)\n","\n","# Split the dataset into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n","\n","# Evaluate performance for different bootstrap sample sizes\n","bootstrap_samples = [0.5, 0.7, 1.0]\n","for bootstrap_size in bootstrap_samples:\n","    # Create and train Bagging Regressor\n","    bagging_regressor = BaggingRegressor(estimator=DecisionTreeRegressor(), n_estimators=50, max_samples=bootstrap_size, random_state=42)\n","    bagging_regressor.fit(X_train, y_train)\n","\n","    # Predict and calculate MSE\n","    y_pred = bagging_regressor.predict(X_test)\n","    mse = mean_squared_error(y_test, y_pred)\n","    print(f\"Bagging Regressor with {bootstrap_size * 100}% Bootstrap Samples - MSE: {mse:.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"49Cthr66mo_u","executionInfo":{"status":"ok","timestamp":1747256566930,"user_tz":-330,"elapsed":376,"user":{"displayName":"dnyanesh fuke","userId":"14164670016989196152"}},"outputId":"88d29c8b-5057-4553-c506-bb6e215da497"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["Bagging Regressor with 50.0% Bootstrap Samples - MSE: 739.5177\n","Bagging Regressor with 70.0% Bootstrap Samples - MSE: 513.2512\n","Bagging Regressor with 100.0% Bootstrap Samples - MSE: 438.8215\n"]}]},{"cell_type":"markdown","source":["---"],"metadata":{"id":"BcwXpka-nfR_"}}]}